{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "from nltk.stem import PorterStemmer\n",
    "my_stem = PorterStemmer()\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "dictionary = set(w.lower() for w in nltk.corpus.words.words())\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "politifact = pd.read_csv(\"raw-data/FakeNewsNet-master/Data/Cleaned_DSPP/politifact.csv\")\n",
    "\n",
    "### remove articles with no text\n",
    "politifact = politifact[~politifact.text.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_df = pd.DataFrame()\n",
    "## clean/prepare text\n",
    "for rev, outcome in zip(politifact.text.tolist(), politifact.fake.tolist()):\n",
    "    \n",
    "    # only keep words (remove other characters)\n",
    "    tmp_read = re.sub('[^a-zA-Z]+', ' ', rev).lower()\n",
    "\n",
    "    #Tokenization and remove stop words\n",
    "    tmp_read = [word for word in tmp_read.split() if word not in stop_words]\n",
    "\n",
    "    #dictionary words\n",
    "    dict_read = [word for word in tmp_read if word in dictionary]\n",
    "    \n",
    "    # stemming\n",
    "    tmp_read_stm = [my_stem.stem(word) for word in tmp_read]\n",
    "    dict_read_stm = [my_stem.stem(word) for word in dict_read]\n",
    "\n",
    "    # lemminization\n",
    "    tmp_read_lem = [lemmatizer.lemmatize(word) for word in tmp_read]\n",
    "    dict_read_lem = [lemmatizer.lemmatize(word) for word in dict_read]\n",
    "\n",
    "    \n",
    "    # rejoin reviews\n",
    "    tmp_read = ' '.join(tmp_read)\n",
    "    tmp_read_stm = ' '.join(tmp_read_stm)\n",
    "    tmp_read_lem = ' '.join(tmp_read_lem)\n",
    "    \n",
    "    dict_read = ' '.join(dict_read)\n",
    "    dict_read_stm = ' '.join(dict_read_stm)\n",
    "    dict_read_lem = ' '.join(dict_read_lem)\n",
    "\n",
    "\n",
    "    # add to new df\n",
    "    tmp = pd.DataFrame([rev], columns=['original text'])\n",
    "    tmp['body'] = tmp_read\n",
    "    tmp['body_stem'] = tmp_read_stm\n",
    "    tmp['body_lem'] = tmp_read_lem\n",
    "    tmp['body_dict'] = dict_read\n",
    "    tmp['body_dict_stem'] = dict_read_stm\n",
    "    tmp['fake'] = outcome\n",
    "\n",
    "    the_df = the_df.append(tmp, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original text</th>\n",
       "      <th>body</th>\n",
       "      <th>body_stem</th>\n",
       "      <th>body_lem</th>\n",
       "      <th>body_dict</th>\n",
       "      <th>body_dict_stem</th>\n",
       "      <th>fake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>335 SHARES SHARE THIS STORY\\n\\nRepublican atta...</td>\n",
       "      <td>shares share story republican attacks transgen...</td>\n",
       "      <td>share share stori republican attack transgend ...</td>\n",
       "      <td>share share story republican attack transgende...</td>\n",
       "      <td>share story republican religious fight keep ge...</td>\n",
       "      <td>share stori republican religi fight keep gende...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BREAKING!\\n\\nLiberal rag Huffington Post is re...</td>\n",
       "      <td>breaking liberal rag huffington post really ru...</td>\n",
       "      <td>break liber rag huffington post realli run sto...</td>\n",
       "      <td>breaking liberal rag huffington post really ru...</td>\n",
       "      <td>breaking liberal rag post really running story...</td>\n",
       "      <td>break liber rag post realli run stori washingt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Three women who all went missing in the mid-19...</td>\n",
       "      <td>three women went missing mid turned least part...</td>\n",
       "      <td>three women went miss mid turn least part stee...</td>\n",
       "      <td>three woman went missing mid turned least part...</td>\n",
       "      <td>three went missing mid turned least steel indu...</td>\n",
       "      <td>three went miss mid turn least steel industri ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On Monday, Bumble Bee Foods and 2 employees we...</td>\n",
       "      <td>monday bumble bee foods employees charged los ...</td>\n",
       "      <td>monday bumbl bee food employe charg lo angel p...</td>\n",
       "      <td>monday bumble bee food employee charged los an...</td>\n",
       "      <td>monday bumble bee safety death worker industri...</td>\n",
       "      <td>monday bumbl bee safeti death worker industri ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Republican Rep. Trey Gowdy, who sits on the Ho...</td>\n",
       "      <td>republican rep trey gowdy sits house judiciary...</td>\n",
       "      <td>republican rep trey gowdi sit hous judiciari c...</td>\n",
       "      <td>republican rep trey gowdy sits house judiciary...</td>\n",
       "      <td>republican rep trey house judiciary committee ...</td>\n",
       "      <td>republican rep trey hous judiciari committe fr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original text  \\\n",
       "0  335 SHARES SHARE THIS STORY\\n\\nRepublican atta...   \n",
       "1  BREAKING!\\n\\nLiberal rag Huffington Post is re...   \n",
       "2  Three women who all went missing in the mid-19...   \n",
       "3  On Monday, Bumble Bee Foods and 2 employees we...   \n",
       "4  Republican Rep. Trey Gowdy, who sits on the Ho...   \n",
       "\n",
       "                                                body  \\\n",
       "0  shares share story republican attacks transgen...   \n",
       "1  breaking liberal rag huffington post really ru...   \n",
       "2  three women went missing mid turned least part...   \n",
       "3  monday bumble bee foods employees charged los ...   \n",
       "4  republican rep trey gowdy sits house judiciary...   \n",
       "\n",
       "                                           body_stem  \\\n",
       "0  share share stori republican attack transgend ...   \n",
       "1  break liber rag huffington post realli run sto...   \n",
       "2  three women went miss mid turn least part stee...   \n",
       "3  monday bumbl bee food employe charg lo angel p...   \n",
       "4  republican rep trey gowdi sit hous judiciari c...   \n",
       "\n",
       "                                            body_lem  \\\n",
       "0  share share story republican attack transgende...   \n",
       "1  breaking liberal rag huffington post really ru...   \n",
       "2  three woman went missing mid turned least part...   \n",
       "3  monday bumble bee food employee charged los an...   \n",
       "4  republican rep trey gowdy sits house judiciary...   \n",
       "\n",
       "                                           body_dict  \\\n",
       "0  share story republican religious fight keep ge...   \n",
       "1  breaking liberal rag post really running story...   \n",
       "2  three went missing mid turned least steel indu...   \n",
       "3  monday bumble bee safety death worker industri...   \n",
       "4  republican rep trey house judiciary committee ...   \n",
       "\n",
       "                                      body_dict_stem  fake  \n",
       "0  share stori republican religi fight keep gende...     1  \n",
       "1  break liber rag post realli run stori washingt...     1  \n",
       "2  three went miss mid turn least steel industri ...     1  \n",
       "3  monday bumbl bee safeti death worker industri ...     1  \n",
       "4  republican rep trey hous judiciari committe fr...     1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_length(the_df):\n",
    "    '''count number of words in each speech'''\n",
    "    l = pd.Series.tolist(the_df[['original text']])\n",
    "    ct = 0\n",
    "    count_per_speech = []\n",
    "    for line in l:\n",
    "        for i in line:\n",
    "            i = i.split()\n",
    "            ct += 1\n",
    "            count_per_speech.append(len(i))\n",
    "    the_df['total_words'] = count_per_speech\n",
    "    return(the_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_stats(the_df):\n",
    "    '''sent_length = num of sentences in speech\n",
    "       num_word_unique = num of unique words in speech'''\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    \n",
    "    sent_len = []\n",
    "    unique_len = []\n",
    "    body_basic = []\n",
    "    dict_len = []\n",
    "    dict_uniq_len = []\n",
    "    punctuations = '''!()-[]{}';:'\"\\,<>./?@#$%^&*_~``'''\n",
    "    ct = 0\n",
    "    for i in the_df['original text']:\n",
    "        sents = sent_tokenize(i)\n",
    "        tmp = word_tokenize(i)\n",
    "        tmp_words = [i.lower() for i in tmp if i not in punctuations]\n",
    "        #dictionary words\n",
    "        dict_read = [word for word in tmp_words if word in dictionary]\n",
    "        dict_unique = len(set(dict_read))\n",
    "        dict_len.append(len(dict_read))\n",
    "        dict_uniq_len.append(dict_unique)\n",
    "        \n",
    "        tmp_unique = len(set(tmp))\n",
    "        tmp_words = ' '.join(tmp_words)\n",
    "        sent_len.append(len(sents))\n",
    "        unique_len.append(tmp_unique)\n",
    "        body_basic.append(tmp_words)\n",
    "\n",
    "    the_df['body_basic'] = body_basic\n",
    "    the_df['sent_length'] = sent_len\n",
    "    the_df['num_word_unique'] = unique_len\n",
    "    the_df['num_dict_word_unique'] = dict_uniq_len\n",
    "    the_df['total_dict_words'] = dict_len\n",
    "    return(the_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_df = article_length(the_df)\n",
    "the_df = summary_stats(the_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original text</th>\n",
       "      <th>body</th>\n",
       "      <th>body_stem</th>\n",
       "      <th>body_lem</th>\n",
       "      <th>body_dict</th>\n",
       "      <th>body_dict_stem</th>\n",
       "      <th>fake</th>\n",
       "      <th>total_words</th>\n",
       "      <th>body_basic</th>\n",
       "      <th>sent_length</th>\n",
       "      <th>num_word_unique</th>\n",
       "      <th>num_dict_word_unique</th>\n",
       "      <th>total_dict_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>335 SHARES SHARE THIS STORY\\n\\nRepublican atta...</td>\n",
       "      <td>shares share story republican attacks transgen...</td>\n",
       "      <td>share share stori republican attack transgend ...</td>\n",
       "      <td>share share story republican attack transgende...</td>\n",
       "      <td>share story republican religious fight keep ge...</td>\n",
       "      <td>share stori republican religi fight keep gende...</td>\n",
       "      <td>1</td>\n",
       "      <td>287</td>\n",
       "      <td>335 shares share this story republican attacks...</td>\n",
       "      <td>8</td>\n",
       "      <td>182</td>\n",
       "      <td>132</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BREAKING!\\n\\nLiberal rag Huffington Post is re...</td>\n",
       "      <td>breaking liberal rag huffington post really ru...</td>\n",
       "      <td>break liber rag huffington post realli run sto...</td>\n",
       "      <td>breaking liberal rag huffington post really ru...</td>\n",
       "      <td>breaking liberal rag post really running story...</td>\n",
       "      <td>break liber rag post realli run stori washingt...</td>\n",
       "      <td>1</td>\n",
       "      <td>367</td>\n",
       "      <td>breaking liberal rag huffington post is really...</td>\n",
       "      <td>16</td>\n",
       "      <td>231</td>\n",
       "      <td>157</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Three women who all went missing in the mid-19...</td>\n",
       "      <td>three women went missing mid turned least part...</td>\n",
       "      <td>three women went miss mid turn least part stee...</td>\n",
       "      <td>three woman went missing mid turned least part...</td>\n",
       "      <td>three went missing mid turned least steel indu...</td>\n",
       "      <td>three went miss mid turn least steel industri ...</td>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>three women who all went missing in the mid-19...</td>\n",
       "      <td>12</td>\n",
       "      <td>219</td>\n",
       "      <td>161</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On Monday, Bumble Bee Foods and 2 employees we...</td>\n",
       "      <td>monday bumble bee foods employees charged los ...</td>\n",
       "      <td>monday bumbl bee food employe charg lo angel p...</td>\n",
       "      <td>monday bumble bee food employee charged los an...</td>\n",
       "      <td>monday bumble bee safety death worker industri...</td>\n",
       "      <td>monday bumbl bee safeti death worker industri ...</td>\n",
       "      <td>1</td>\n",
       "      <td>258</td>\n",
       "      <td>on monday bumble bee foods and 2 employees wer...</td>\n",
       "      <td>11</td>\n",
       "      <td>167</td>\n",
       "      <td>105</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Republican Rep. Trey Gowdy, who sits on the Ho...</td>\n",
       "      <td>republican rep trey gowdy sits house judiciary...</td>\n",
       "      <td>republican rep trey gowdi sit hous judiciari c...</td>\n",
       "      <td>republican rep trey gowdy sits house judiciary...</td>\n",
       "      <td>republican rep trey house judiciary committee ...</td>\n",
       "      <td>republican rep trey hous judiciari committe fr...</td>\n",
       "      <td>1</td>\n",
       "      <td>563</td>\n",
       "      <td>republican rep. trey gowdy who sits on the hou...</td>\n",
       "      <td>20</td>\n",
       "      <td>294</td>\n",
       "      <td>204</td>\n",
       "      <td>454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original text  \\\n",
       "0  335 SHARES SHARE THIS STORY\\n\\nRepublican atta...   \n",
       "1  BREAKING!\\n\\nLiberal rag Huffington Post is re...   \n",
       "2  Three women who all went missing in the mid-19...   \n",
       "3  On Monday, Bumble Bee Foods and 2 employees we...   \n",
       "4  Republican Rep. Trey Gowdy, who sits on the Ho...   \n",
       "\n",
       "                                                body  \\\n",
       "0  shares share story republican attacks transgen...   \n",
       "1  breaking liberal rag huffington post really ru...   \n",
       "2  three women went missing mid turned least part...   \n",
       "3  monday bumble bee foods employees charged los ...   \n",
       "4  republican rep trey gowdy sits house judiciary...   \n",
       "\n",
       "                                           body_stem  \\\n",
       "0  share share stori republican attack transgend ...   \n",
       "1  break liber rag huffington post realli run sto...   \n",
       "2  three women went miss mid turn least part stee...   \n",
       "3  monday bumbl bee food employe charg lo angel p...   \n",
       "4  republican rep trey gowdi sit hous judiciari c...   \n",
       "\n",
       "                                            body_lem  \\\n",
       "0  share share story republican attack transgende...   \n",
       "1  breaking liberal rag huffington post really ru...   \n",
       "2  three woman went missing mid turned least part...   \n",
       "3  monday bumble bee food employee charged los an...   \n",
       "4  republican rep trey gowdy sits house judiciary...   \n",
       "\n",
       "                                           body_dict  \\\n",
       "0  share story republican religious fight keep ge...   \n",
       "1  breaking liberal rag post really running story...   \n",
       "2  three went missing mid turned least steel indu...   \n",
       "3  monday bumble bee safety death worker industri...   \n",
       "4  republican rep trey house judiciary committee ...   \n",
       "\n",
       "                                      body_dict_stem  fake  total_words  \\\n",
       "0  share stori republican religi fight keep gende...     1          287   \n",
       "1  break liber rag post realli run stori washingt...     1          367   \n",
       "2  three went miss mid turn least steel industri ...     1          337   \n",
       "3  monday bumbl bee safeti death worker industri ...     1          258   \n",
       "4  republican rep trey hous judiciari committe fr...     1          563   \n",
       "\n",
       "                                          body_basic  sent_length  \\\n",
       "0  335 shares share this story republican attacks...            8   \n",
       "1  breaking liberal rag huffington post is really...           16   \n",
       "2  three women who all went missing in the mid-19...           12   \n",
       "3  on monday bumble bee foods and 2 employees wer...           11   \n",
       "4  republican rep. trey gowdy who sits on the hou...           20   \n",
       "\n",
       "   num_word_unique  num_dict_word_unique  total_dict_words  \n",
       "0              182                   132               245  \n",
       "1              231                   157               302  \n",
       "2              219                   161               292  \n",
       "3              167                   105               195  \n",
       "4              294                   204               454  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>source</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>fake</th>\n",
       "      <th>date_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PolitiFact_Fake_1-Webpage.json</td>\n",
       "      <td>http://www.occupydemocrats.com</td>\n",
       "      <td>2016-01-12</td>\n",
       "      <td>Virginia Republican Wants Schools To Check Chi...</td>\n",
       "      <td>335 SHARES SHARE THIS STORY\\n\\nRepublican atta...</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-12 15:02:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PolitiFact_Fake_10-Webpage.json</td>\n",
       "      <td>http://usasnich.com</td>\n",
       "      <td>2016-12-11</td>\n",
       "      <td>BREAKING: PUTIN INTERFERENCE COULD GIVE COURTS...</td>\n",
       "      <td>BREAKING!\\n\\nLiberal rag Huffington Post is re...</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-12-11 13:03:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PolitiFact_Fake_100-Webpage.json</td>\n",
       "      <td>http://freedomcrossroads.us</td>\n",
       "      <td>2017-06-20</td>\n",
       "      <td>BREAKING: Barrels Removed From Clinton Propert...</td>\n",
       "      <td>Three women who all went missing in the mid-19...</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-06-20 16:34:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PolitiFact_Fake_101-Webpage.json</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>On Monday, Bumble Bee Foods and 2 employees we...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PolitiFact_Fake_102-Webpage.json</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Republican Rep. Trey Gowdy, who sits on the Ho...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Unnamed: 0                          source  \\\n",
       "0    PolitiFact_Fake_1-Webpage.json  http://www.occupydemocrats.com   \n",
       "1   PolitiFact_Fake_10-Webpage.json             http://usasnich.com   \n",
       "2  PolitiFact_Fake_100-Webpage.json     http://freedomcrossroads.us   \n",
       "3  PolitiFact_Fake_101-Webpage.json                             NaN   \n",
       "4  PolitiFact_Fake_102-Webpage.json                             NaN   \n",
       "\n",
       "         date                                              title  \\\n",
       "0  2016-01-12  Virginia Republican Wants Schools To Check Chi...   \n",
       "1  2016-12-11  BREAKING: PUTIN INTERFERENCE COULD GIVE COURTS...   \n",
       "2  2017-06-20  BREAKING: Barrels Removed From Clinton Propert...   \n",
       "3         NaN                                                NaN   \n",
       "4         NaN                                                NaN   \n",
       "\n",
       "                                                text  fake  \\\n",
       "0  335 SHARES SHARE THIS STORY\\n\\nRepublican atta...     1   \n",
       "1  BREAKING!\\n\\nLiberal rag Huffington Post is re...     1   \n",
       "2  Three women who all went missing in the mid-19...     1   \n",
       "3  On Monday, Bumble Bee Foods and 2 employees we...     1   \n",
       "4  Republican Rep. Trey Gowdy, who sits on the Ho...     1   \n",
       "\n",
       "             date_time  \n",
       "0  2016-01-12 15:02:28  \n",
       "1  2016-12-11 13:03:24  \n",
       "2  2017-06-20 16:34:03  \n",
       "3                  NaN  \n",
       "4                  NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politifact.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rejoin and sort by date\n",
    "output = the_df.merge(politifact[[\"text\",\"date\"]], left_on='original text',right_on=\"text\", how='left').sort_values(\"date\").reset_index()\n",
    "# output.to_csv(\"./output-data/output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_words</th>\n",
       "      <th>sent_length</th>\n",
       "      <th>num_word_unique</th>\n",
       "      <th>num_dict_word_unique</th>\n",
       "      <th>total_dict_words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fake</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>642.266667</td>\n",
       "      <td>27.625000</td>\n",
       "      <td>321.358333</td>\n",
       "      <td>222.283333</td>\n",
       "      <td>541.991667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>394.872881</td>\n",
       "      <td>17.567797</td>\n",
       "      <td>220.322034</td>\n",
       "      <td>159.008475</td>\n",
       "      <td>341.550847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      total_words  sent_length  num_word_unique  num_dict_word_unique  \\\n",
       "fake                                                                    \n",
       "0      642.266667    27.625000       321.358333            222.283333   \n",
       "1      394.872881    17.567797       220.322034            159.008475   \n",
       "\n",
       "      total_dict_words  \n",
       "fake                    \n",
       "0           541.991667  \n",
       "1           341.550847  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df_avg = the_df.groupby(['fake']).mean()\n",
    "grouped_df_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prop_uniq_dict - of dictionary words in an article, what proportion of the words are unique?\n",
    "grouped_df_avg['prop_uniq_dict'] = grouped_df_avg['num_dict_word_unique'] / grouped_df_avg['total_dict_words']\n",
    "# prop_uniq - of total words in an article, what proportion of the words are unique?\n",
    "grouped_df_avg['prop_uniq'] = grouped_df_avg['num_word_unique'] / grouped_df_avg['total_words']\n",
    "# prop_dict_words - of total words in an article, what proportion of the words are in a dictionary?\n",
    "grouped_df_avg['prop_dict_words'] = grouped_df_avg['total_dict_words'] / grouped_df_avg['total_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_words</th>\n",
       "      <th>sent_length</th>\n",
       "      <th>num_word_unique</th>\n",
       "      <th>num_dict_word_unique</th>\n",
       "      <th>total_dict_words</th>\n",
       "      <th>prop_uniq_dict</th>\n",
       "      <th>prop_uniq</th>\n",
       "      <th>prop_dict_words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fake</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>642.266667</td>\n",
       "      <td>27.625000</td>\n",
       "      <td>321.358333</td>\n",
       "      <td>222.283333</td>\n",
       "      <td>541.991667</td>\n",
       "      <td>0.410123</td>\n",
       "      <td>0.500350</td>\n",
       "      <td>0.843873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>394.872881</td>\n",
       "      <td>17.567797</td>\n",
       "      <td>220.322034</td>\n",
       "      <td>159.008475</td>\n",
       "      <td>341.550847</td>\n",
       "      <td>0.465548</td>\n",
       "      <td>0.557957</td>\n",
       "      <td>0.864964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      total_words  sent_length  num_word_unique  num_dict_word_unique  \\\n",
       "fake                                                                    \n",
       "0      642.266667    27.625000       321.358333            222.283333   \n",
       "1      394.872881    17.567797       220.322034            159.008475   \n",
       "\n",
       "      total_dict_words  prop_uniq_dict  prop_uniq  prop_dict_words  \n",
       "fake                                                                \n",
       "0           541.991667        0.410123   0.500350         0.843873  \n",
       "1           341.550847        0.465548   0.557957         0.864964  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## null hypothesis: number of unique works of fake and real articles \n",
    "## are the same\n",
    "\n",
    "### https://machinelearningmastery.com/how-to-code-the-students-t-test-from-scratch-in-python/\n",
    "def independent_ttest(data1, data2, alpha= 0.05):\n",
    "    from math import sqrt\n",
    "    from numpy.random import seed\n",
    "    from numpy.random import randn\n",
    "    from numpy import mean\n",
    "    from scipy.stats import sem\n",
    "    from scipy.stats import t\n",
    "    # calculate means\n",
    "    mean1, mean2 = mean(data1), mean(data2)\n",
    "    # calculate standard errors\n",
    "    se1, se2 = sem(data1), sem(data2)\n",
    "    # standard error on the difference between the samples\n",
    "    sed = sqrt(se1**2.0 + se2**2.0)\n",
    "    # calculate the t statistic\n",
    "    t_stat = (mean1 - mean2) / sed\n",
    "    # degrees of freedom\n",
    "    df = len(data1) + len(data2) - 2\n",
    "    # calculate the critical value\n",
    "    cv = t.ppf(1.0 - alpha, df)\n",
    "    # calculate the p-value\n",
    "    p = (1.0 - t.cdf(abs(t_stat), df)) * 2.0\n",
    "    # return everything\n",
    "    return t_stat, df, cv, p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**statistically significant difference in the number of unique words in fake vs real news articles.** We can reject the null hypothesis that the means of unique words in these articles are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0009826839274891253"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = the_df[the_df.fake==0].num_word_unique.tolist()\n",
    "data2 = the_df[the_df.fake==1].num_word_unique.tolist()\n",
    "\n",
    "t_stat, df, cv, p = independent_ttest(data1, data2)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**statistically significant difference in the length of sentences in fake vs real news articles.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004532788546822708"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = the_df[the_df.fake==0].sent_length.tolist()\n",
    "data2 = the_df[the_df.fake==1].sent_length.tolist()\n",
    "\n",
    "t_stat, df, cv, p = independent_ttest(data1, data2)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**statistically significant difference in the article lengths in fake vs real news articles.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0019798857071273712"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = the_df[the_df.fake==0].article_length.tolist()\n",
    "data2 = the_df[the_df.fake==1].article_length.tolist()\n",
    "\n",
    "t_stat, df, cv, p = independent_ttest(data1, data2)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**standardizing -- proportion of the article**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prop_uniq_dict - of dictionary words in an article, what proportion of the words are unique?\n",
    "the_df['prop_uniq_dict'] = the_df['num_dict_word_unique'] / the_df['total_dict_words']\n",
    "# prop_uniq - of total words in an article, what proportion of the words are unique?\n",
    "the_df['prop_uniq'] = the_df['num_word_unique'] / the_df['total_words']\n",
    "# prop_dict_words - of total words in an article, what proportion of the words are in a dictionary?\n",
    "the_df['prop_dict_words'] = the_df['total_dict_words'] / the_df['total_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**statistically significant difference in the proportion of unique dictionary words in fake vs real news articles.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000233015652925328"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = the_df[the_df.fake==0].prop_uniq_dict.tolist()\n",
    "data2 = the_df[the_df.fake==1].prop_uniq_dict.tolist()\n",
    "\n",
    "t_stat, df, cv, p = independent_ttest(data1, data2)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**statistically significant difference in the proportion of unique words in fake vs real news articles.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00028340517072944493"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = the_df[the_df.fake==0].prop_uniq.tolist()\n",
    "data2 = the_df[the_df.fake==1].prop_uniq.tolist()\n",
    "\n",
    "t_stat, df, cv, p = independent_ttest(data1, data2)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**no statistically significant difference in the proportion of total dictionary words in fake vs real news articles.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07454492366895393"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = the_df[the_df.fake==0].prop_dict_words.tolist()\n",
    "data2 = the_df[the_df.fake==1].prop_dict_words.tolist()\n",
    "\n",
    "t_stat, df, cv, p = independent_ttest(data1, data2)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original text</th>\n",
       "      <th>body</th>\n",
       "      <th>body_stem</th>\n",
       "      <th>body_lem</th>\n",
       "      <th>body_dict</th>\n",
       "      <th>body_dict_stem</th>\n",
       "      <th>fake</th>\n",
       "      <th>article_length</th>\n",
       "      <th>body_basic</th>\n",
       "      <th>sent_length</th>\n",
       "      <th>num_word_unique</th>\n",
       "      <th>total_words</th>\n",
       "      <th>num_dict_word_unique</th>\n",
       "      <th>total_dict_words</th>\n",
       "      <th>prop_uniq_dict</th>\n",
       "      <th>prop_uniq</th>\n",
       "      <th>prop_dict_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>335 SHARES SHARE THIS STORY\\n\\nRepublican atta...</td>\n",
       "      <td>shares share story republican attacks transgen...</td>\n",
       "      <td>share share stori republican attack transgend ...</td>\n",
       "      <td>share share story republican attack transgende...</td>\n",
       "      <td>share story republican religious fight keep ge...</td>\n",
       "      <td>share stori republican religi fight keep gende...</td>\n",
       "      <td>1</td>\n",
       "      <td>287</td>\n",
       "      <td>335 shares share this story republican attacks...</td>\n",
       "      <td>8</td>\n",
       "      <td>182</td>\n",
       "      <td>287</td>\n",
       "      <td>132</td>\n",
       "      <td>245</td>\n",
       "      <td>0.538776</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.853659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BREAKING!\\n\\nLiberal rag Huffington Post is re...</td>\n",
       "      <td>breaking liberal rag huffington post really ru...</td>\n",
       "      <td>break liber rag huffington post realli run sto...</td>\n",
       "      <td>breaking liberal rag huffington post really ru...</td>\n",
       "      <td>breaking liberal rag post really running story...</td>\n",
       "      <td>break liber rag post realli run stori washingt...</td>\n",
       "      <td>1</td>\n",
       "      <td>367</td>\n",
       "      <td>breaking liberal rag huffington post is really...</td>\n",
       "      <td>16</td>\n",
       "      <td>231</td>\n",
       "      <td>367</td>\n",
       "      <td>157</td>\n",
       "      <td>302</td>\n",
       "      <td>0.519868</td>\n",
       "      <td>0.629428</td>\n",
       "      <td>0.822888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Three women who all went missing in the mid-19...</td>\n",
       "      <td>three women went missing mid turned least part...</td>\n",
       "      <td>three women went miss mid turn least part stee...</td>\n",
       "      <td>three woman went missing mid turned least part...</td>\n",
       "      <td>three went missing mid turned least steel indu...</td>\n",
       "      <td>three went miss mid turn least steel industri ...</td>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>three women who all went missing in the mid-19...</td>\n",
       "      <td>12</td>\n",
       "      <td>219</td>\n",
       "      <td>337</td>\n",
       "      <td>161</td>\n",
       "      <td>292</td>\n",
       "      <td>0.551370</td>\n",
       "      <td>0.649852</td>\n",
       "      <td>0.866469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On Monday, Bumble Bee Foods and 2 employees we...</td>\n",
       "      <td>monday bumble bee foods employees charged los ...</td>\n",
       "      <td>monday bumbl bee food employe charg lo angel p...</td>\n",
       "      <td>monday bumble bee food employee charged los an...</td>\n",
       "      <td>monday bumble bee safety death worker industri...</td>\n",
       "      <td>monday bumbl bee safeti death worker industri ...</td>\n",
       "      <td>1</td>\n",
       "      <td>258</td>\n",
       "      <td>on monday bumble bee foods and 2 employees wer...</td>\n",
       "      <td>11</td>\n",
       "      <td>167</td>\n",
       "      <td>258</td>\n",
       "      <td>105</td>\n",
       "      <td>195</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.647287</td>\n",
       "      <td>0.755814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Republican Rep. Trey Gowdy, who sits on the Ho...</td>\n",
       "      <td>republican rep trey gowdy sits house judiciary...</td>\n",
       "      <td>republican rep trey gowdi sit hous judiciari c...</td>\n",
       "      <td>republican rep trey gowdy sits house judiciary...</td>\n",
       "      <td>republican rep trey house judiciary committee ...</td>\n",
       "      <td>republican rep trey hous judiciari committe fr...</td>\n",
       "      <td>1</td>\n",
       "      <td>563</td>\n",
       "      <td>republican rep. trey gowdy who sits on the hou...</td>\n",
       "      <td>20</td>\n",
       "      <td>294</td>\n",
       "      <td>563</td>\n",
       "      <td>204</td>\n",
       "      <td>454</td>\n",
       "      <td>0.449339</td>\n",
       "      <td>0.522202</td>\n",
       "      <td>0.806394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original text  \\\n",
       "0  335 SHARES SHARE THIS STORY\\n\\nRepublican atta...   \n",
       "1  BREAKING!\\n\\nLiberal rag Huffington Post is re...   \n",
       "2  Three women who all went missing in the mid-19...   \n",
       "3  On Monday, Bumble Bee Foods and 2 employees we...   \n",
       "4  Republican Rep. Trey Gowdy, who sits on the Ho...   \n",
       "\n",
       "                                                body  \\\n",
       "0  shares share story republican attacks transgen...   \n",
       "1  breaking liberal rag huffington post really ru...   \n",
       "2  three women went missing mid turned least part...   \n",
       "3  monday bumble bee foods employees charged los ...   \n",
       "4  republican rep trey gowdy sits house judiciary...   \n",
       "\n",
       "                                           body_stem  \\\n",
       "0  share share stori republican attack transgend ...   \n",
       "1  break liber rag huffington post realli run sto...   \n",
       "2  three women went miss mid turn least part stee...   \n",
       "3  monday bumbl bee food employe charg lo angel p...   \n",
       "4  republican rep trey gowdi sit hous judiciari c...   \n",
       "\n",
       "                                            body_lem  \\\n",
       "0  share share story republican attack transgende...   \n",
       "1  breaking liberal rag huffington post really ru...   \n",
       "2  three woman went missing mid turned least part...   \n",
       "3  monday bumble bee food employee charged los an...   \n",
       "4  republican rep trey gowdy sits house judiciary...   \n",
       "\n",
       "                                           body_dict  \\\n",
       "0  share story republican religious fight keep ge...   \n",
       "1  breaking liberal rag post really running story...   \n",
       "2  three went missing mid turned least steel indu...   \n",
       "3  monday bumble bee safety death worker industri...   \n",
       "4  republican rep trey house judiciary committee ...   \n",
       "\n",
       "                                      body_dict_stem  fake  article_length  \\\n",
       "0  share stori republican religi fight keep gende...     1             287   \n",
       "1  break liber rag post realli run stori washingt...     1             367   \n",
       "2  three went miss mid turn least steel industri ...     1             337   \n",
       "3  monday bumbl bee safeti death worker industri ...     1             258   \n",
       "4  republican rep trey hous judiciari committe fr...     1             563   \n",
       "\n",
       "                                          body_basic  sent_length  \\\n",
       "0  335 shares share this story republican attacks...            8   \n",
       "1  breaking liberal rag huffington post is really...           16   \n",
       "2  three women who all went missing in the mid-19...           12   \n",
       "3  on monday bumble bee foods and 2 employees wer...           11   \n",
       "4  republican rep. trey gowdy who sits on the hou...           20   \n",
       "\n",
       "   num_word_unique  total_words  num_dict_word_unique  total_dict_words  \\\n",
       "0              182          287                   132               245   \n",
       "1              231          367                   157               302   \n",
       "2              219          337                   161               292   \n",
       "3              167          258                   105               195   \n",
       "4              294          563                   204               454   \n",
       "\n",
       "   prop_uniq_dict  prop_uniq  prop_dict_words  \n",
       "0        0.538776   0.634146         0.853659  \n",
       "1        0.519868   0.629428         0.822888  \n",
       "2        0.551370   0.649852         0.866469  \n",
       "3        0.538462   0.647287         0.755814  \n",
       "4        0.449339   0.522202         0.806394  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part of Speech analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_adj = ['JJ', 'JJR', 'JJS']\n",
    "the_verb = ['VB', 'VBD', 'VGB', 'VBN', 'VBP', 'VBZ']\n",
    "adj_set_real = []\n",
    "adj_set_fake = []\n",
    "\n",
    "for rev, outcome in zip(politifact.text.tolist(), politifact.fake.tolist()):\n",
    "    \n",
    "    # only keep words (remove other characters)\n",
    "    tmp_read = re.sub('[^a-zA-Z]+', ' ', rev).lower()\n",
    "\n",
    "    #Tokenization and remove stop words\n",
    "    tmp_read = [word for word in tmp_read.split() if word not in stop_words]\n",
    "\n",
    "    pos = nltk.pos_tag(tmp_read)\n",
    "    for w in pos:\n",
    "        if nltk.pos_tag(w)[0][1] in the_adj and outcome==1:\n",
    "            adj_set_fake.add(w[0])\n",
    "        elif nltk.pos_tag(w)[0][1]  in the_adj and outcome==0:\n",
    "            adj_set_real.add(w[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more to explore here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
